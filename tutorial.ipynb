{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark Tutorial\n",
    "\n",
    "## What is PySpark?\n",
    "\n",
    "[https://medium.com/analytics-vidhya/how-does-pyspark-work-step-by-step-with-pictures-c011402ccd57](https://medium.com/analytics-vidhya/how-does-pyspark-work-step-by-step-with-pictures-c011402ccd57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_PYTHON=./env/bin/python\n",
      "env: SPARK_LOCAL_DIRS=/mnt/storage/grid/home/adam/spark_tmp\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_PYTHON=./env/bin/python\n",
    "%env SPARK_LOCAL_DIRS=/mnt/storage/grid/home/adam/spark_tmp\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start a Spark Session\n",
    "\n",
    "The non-VCS files are located at `/home/adam/spark-tutorial` on `magarveylab-computational`.\n",
    "\n",
    "Start a Spark Session to connect to the Spark cluster from the python client.\n",
    "Additional configuration options are available in the Apache Spark documentation.\n",
    "See more at [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html).\n",
    "Two options of importance are driver and executor memory,\n",
    "which are the per-node memory allocations for the Spark Session.\n",
    "The driver node performs aggregate operations across partitions,\n",
    "while executor nodes only perform narrow operations.\n",
    "In order to make dependencies, package the active conda environment\n",
    "with `conda pack -o -f env.tar.gz` and update the configuration.\n",
    "The spark archives convention is `<path>#<alias>`,\n",
    "where the `<path>` is the path to the archive on the client machine,\n",
    "and the `<alias>` maps to `./<alias>` on the executor machine.\n",
    "For example, `env.tar.gz#env` is accessed on the executor via `./env`,\n",
    "as is seen applied in `%env PYSPARK_PYTHON=./env/bin/python`.\n",
    "When the disk space on the executor is not sufficient for `spark.archives`,\n",
    "map the working directory of the executor to where there is space,\n",
    "such as a network drive `%env SPARK_LOCAL_DIRS=/mnt/storage/grid/home/adam/spark_tmp`.\n",
    "Checkout the Spark UI (on port 4040 and greater) to observe\n",
    "task progress, computational graph, and resource utilization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x7fdd8d72aa30>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://magarveylab-computational.csu.mcmaster.ca:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://MagarveyLab-mn1:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>spark-tutorial</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .appName(\"spark-tutorial\")\n",
    "    .master(\"spark://MagarveyLab-mn1:7077\")\n",
    "    .config(\"spark.archives\", \"env.tar.gz#env,rxnmapper-model-albert.tar.gz#rxnmapper-model-albert\")\n",
    "    .config(\"spark.dynamicAllocation.enable\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\n",
    "    .config(\"spark.driver.memory\", \"64g\")\n",
    "    .config(\"spark.executor.memory\", \"64g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "\n",
    "Load a directory of JSON files in a single line.\n",
    "Data is loaded from disk into a Resilient Distributed Dataset (RDD),\n",
    "which is the fundamental data abstraction for Apache Spark.\n",
    "The interface layer built on RDD is the Spark Dataframe\n",
    "and enables a convenient and performant interface to manipulate the underlying RDD.\n",
    "The Spark Dataframe interface is similar to Pandas Dataframe.\n",
    "Use `.cache()` on the current task when there are multiple downstream tasks that depend on the current task.\n",
    "Here, operations described in `Glance Dataset` all depend on `df` so we cache it to avoid loading from disk many times."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"/mnt/storage/grid/home/gunam/data_sets/ms2/theoretical_fragmentations\").cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Glance Dataset\n",
    "\n",
    "1. Examine dataset structure using `df.printSchema()`\n",
    "2. Sample some rows using `df.show()`\n",
    "3. Observe summary statistics using `df.summary().show()`\n",
    "\n",
    "Each top-level item in the schema corresponds to a column in the dataset.\n",
    "Items in nested levels represent the object structure of each cell of the column.\n",
    "For more methods, see [https://spark.apache.org/docs/latest/api/python/reference/index.html](https://spark.apache.org/docs/latest/api/python/reference/index.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show()\n",
    "df.summary().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count non-null values in `smiles` series"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.select(\"smiles\").dropna().count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## User Defined Function (UDF)\n",
    "\n",
    "Arbitrary functions that capture serializable and process-safe objects can be mapped over datasets using UDF definitions.\n",
    "In PySpark, UDFs are defined using a decorator, and additionally require the function return type to be specified."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@udf(StructType([\n",
    "    StructField(\"input_ids\", ArrayType(ShortType(), False), False),\n",
    "    StructField(\"token_type_ids\", ArrayType(ShortType(), False), False),\n",
    "    StructField(\"attention_mask\", ArrayType(ShortType(), False), False),\n",
    "]))\n",
    "def encode(smiles):\n",
    "    import rdkit.Chem\n",
    "    import re\n",
    "    import transformers\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "        \"rxnmapper-model-albert\",\n",
    "        do_lower_case=False,\n",
    "        do_basic_tokenize=False,\n",
    "    )\n",
    "    __smiles_regex = re.compile(r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\")\n",
    "    make_words = __smiles_regex.findall\n",
    "    normalize = rdkit.Chem.CanonSmiles\n",
    "    return dict(**tokenizer(make_words(normalize(smiles))))\n",
    "df = df.withColumn(\"encoding\", encode(\"smiles\"))\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resources\n",
    "\n",
    "* PySpark concepts explained by examples [https://sparkbyexamples.com/category/pyspark/](https://sparkbyexamples.com/category/pyspark/)\n",
    "* PySpark official documentation [https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)\n",
    "* Spark SQL documentation [https://spark.apache.org/docs/latest/api/sql/index.html](https://spark.apache.org/docs/latest/api/sql/index.html)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}